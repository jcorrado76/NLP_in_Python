{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "***\n",
    "Our objective here is to classify whether restaurant reviews on Yelp are positive or negative using a perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "***\n",
    "Here are the imports necessary for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Vectorization Classes\n",
    "***\n",
    "These classes are used to transform text data into a vectorized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Vocabulary`\n",
    "***\n",
    "The `Vocabulary` object is used for managing the bijection between each token to a numerical version of itself. \n",
    "\n",
    "The user can add new tokens with the index autoincrementing. We allow for there to be a \"UNK\" token, which stands for the \"unknown\" tokens. This is useful for handling tokens that were never seen in training. \n",
    "\n",
    "We can use `add_token()` to add new tokens to the `Vocabulary`, `lookup_token()` to retrieve the index for a token, and `lookup_index()` to retrieve a token given the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     4,
     30,
     39,
     49,
     71,
     96,
     109,
     112
    ]
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Class to process text and extract Vocabulary for mapping\n",
    "    \"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict) - a map from tokens to indices\n",
    "            add_unk (bool) - a flag that indicates whether to add the UNK token\n",
    "            unk_token (str) - the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        # if no dictionary passed\n",
    "        if token_to_idx is None:\n",
    "            # create one\n",
    "            token_to_idx = {}\n",
    "        # store token_to_idx\n",
    "        self._token_to_idx = token_to_idx\n",
    "        # invert dictionary to have an idx -> token mapping\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        # whether or not to keep a token for unknown words\n",
    "        self._add_unk = add_unk\n",
    "        # token to be used for storing unknown words\n",
    "        self._unk_token = unk_token\n",
    "        # index corresponding to unknown token\n",
    "        self.unk_index = -1\n",
    "        # if we are using a token for unknown words\n",
    "        if add_unk:\n",
    "            # add the unknown token to dict and save its index\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        returns a dictionary that can be serialized\n",
    "        \"\"\"\n",
    "        return {\"token_to_idx\":self._token_to_idx, \\ \n",
    "                \"add_unk\":self._add_unk, \\\n",
    "                \"unk_token\":self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        instantiates the Vocabulary from a serialized dictionary\n",
    "        Args:\n",
    "            contents (dict): a dictionary of the form returned by to_serializable\n",
    "        Returns:\n",
    "            an instance of Vocabulary initialized with contents\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        updates the mapping dict based on the token\n",
    "        \n",
    "        Args:\n",
    "            token - (str) the item to add into the vocabulary\n",
    "        Returns:\n",
    "            index - (int) the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        # if token already added\n",
    "        if token in self._token_to_idx:\n",
    "            # return its index\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            # compute next available index for new token\n",
    "            index = len(self._token_to_idx)\n",
    "            # update token -> idx mapping\n",
    "            self._token_to_idx[token] = index\n",
    "            # update idx -> token mapping \n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"\n",
    "        add a list of tokens into the vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens - (list) a list of tokens as strings\n",
    "        Returns:\n",
    "            indices- (list) a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        retrieve the index associated with the token or the UNK index if the token isn't present\n",
    "        \n",
    "        Args:\n",
    "            token - (str) the token to look up\n",
    "        Returns:\n",
    "            index - (int) the index corresponding to the token\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"\n",
    "        return the token associated with the index\n",
    "        \n",
    "        Args:\n",
    "            index - (int) the index to look up\n",
    "        Returns:\n",
    "            token - (str) the token corresponding to the index\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index {0:d} is not in the Vocabulary\".format(index))\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(\"Vocabulary(size={0:d})\".format(len(self)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ReviewVectorizer`\n",
    "***\n",
    "The second stage of going from a text dataset to a vectorized minibatch is to iterate through the tokens of an input data point and convert each token to its integer form. The result of this iteration should be a vector. Because this vector will be combined with vectors from other data points, there is a constraint that the vectors produced by the `Vectorizer` should always have the same length. \n",
    "\n",
    "The `Vectorizer` must encapsulate the review `Vocabulary`, which maps words in the review to integers. \n",
    "\n",
    "The `Vectorizer` uses `@classmethod` decorator for `from_dataframe()` to indicate an entry point to instantiating the `Vectorizer`. \n",
    "\n",
    "`from_dataframe()` iterates over the rows of a Pandas DataFrame in order to count the frequency of all tokens in the dataset and to create a `Vocabulary` that only uses tokens that are as frequent as some specified frequency `cutoff`.\n",
    "\n",
    "`vectorize()` encapsulates the core functionality of the `Vectorizer`. It takes a review and returns a vectorized representation of the review. In this example, we will be using the one-hot representation for our vectors. \n",
    "\n",
    "This representation creates a binary vector that has length equal to the size of the vocabulary. This representation has some limitations - it is sparse (the number of unique words in a given single review will always be much smaller than the number of unique words in the `Vocabulary`) and it discards any information about the order of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     4,
     13,
     33,
     64,
     78
    ]
   },
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\"\n",
    "    coordinates the vocabularies and uses them\n",
    "    \"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab - (Vocabulary) maps words to integers\n",
    "            rating_vocab - (Vocabulary) maps class labels to integers\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "        \n",
    "    def vectorize(self, review):\n",
    "        \"\"\"\n",
    "        creates a collapsed one-hot vector for the review\n",
    "        \n",
    "        inputs:\n",
    "        review - (str) the review\n",
    "        outputs:\n",
    "        one_hot - (np.array) the collapsed one-hot encoding\n",
    "        \"\"\"\n",
    "        # zeros the length of your vocabulary\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        # for each word\n",
    "        for token in review.split(\" \"):\n",
    "            # if it's not punctuation\n",
    "            if token not in string.punctuation:\n",
    "                # lookup_token retrieves the index corresponding to that token\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\"\n",
    "        instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            review_df - (DataFrame) review dataset\n",
    "            cutoff - (int) the parameter for frquency-based filtering\n",
    "        Returns:\n",
    "            ReviewVectorizer instance\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # add ratings\n",
    "        for rating in sorted(set(review_df[\"rating\"])):\n",
    "            rating_vocab.add_token(rating)\n",
    "            \n",
    "        # add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for review in review_df[\"review\"]:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "                    \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        instantiate a ReviewVectorizer from a serializable dictionary\n",
    "        \n",
    "        Args:\n",
    "            contents - (dict) the serializable dictionary\n",
    "        Returns:\n",
    "            ReviewVectorizer instance\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        \n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        create the serializable dictionary for caching\n",
    "        \n",
    "        outputs:\n",
    "        contents - (dict) the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {\"review_vocab\": self.review_vocab.to_serializable(), \\\n",
    "                \"rating_vocab\": self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ReviewDataset`\n",
    "***\n",
    "The ReviewDataset assumes that the dataset has been minimally cleaned and split into three partitions. It also assumes it can split reviews based on whitespace to get the tokens in a review. In addition, it assumes that the data has an annotation for which split it belongs to. This class inherits from PyTorch's Dataset class in order to provide an API for PyTorch's utilities to work with the dataset. In order to do this, we need to implement the `__getitem__` and `__len__` methods to provide the PyTorch utilities to work with our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0,
     1,
     26,
     38,
     41,
     51,
     54,
     68,
     80,
     96,
     108
    ]
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame) - the dataset\n",
    "            vectorizer (ReviewVectorizer) - vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df['split'] == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df[\"split\"] == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df[\"split\"] == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\"train\": (self.train_df, self.train_size),\n",
    "                             \"val\"  : (self.val_df, self.validation_size),\n",
    "                             \"test\" : (self.test_df, self.test_size)}\n",
    "        # select the split in the dataset to be the training set\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"\n",
    "        load dataset and make new vectorizer\n",
    "\n",
    "        Args:\n",
    "            review_csv - (str) location of dataset\n",
    "        Returns:\n",
    "            instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"\n",
    "        selects the splits in the dataset using a column in the dataframe\n",
    "\n",
    "        Args:\n",
    "            split - (str) one of 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        primary entrypoint for PyTorch Dataset API\n",
    "        \n",
    "        Args:\n",
    "            index - (int) index to the data point\n",
    "        Returns:\n",
    "            a dictionary holding the data points features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        return {\"x_data\": review_vector, \"y_target\": rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        given a bach size, return number of batches in dataset\n",
    "\n",
    "        Args:\n",
    "            batch_size - (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        this loads the dataset, as well as the corresponding vectorizer\n",
    "        this is used after the vectorizer has been cached for repeated use \n",
    "\n",
    "        inputs:\n",
    "        review_csv - (str) location of dataset\n",
    "        vectorizer_filepath - (str) location of saved vectorizer\n",
    "        outputs:\n",
    "        an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        static method for loading the vectorizer from file\n",
    "\n",
    "        inputs:\n",
    "        vectorizer_filepath - (str) location of serialized vectorizer\n",
    "        outputs:\n",
    "        instance fo ReviewVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        save vectorizer to disk using json\n",
    "\n",
    "        inputs:\n",
    "        vectorizer_filepath - (str) location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serialzable(), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "***\n",
    "The final stage of the text-to-vectorized-minibatch pipeline is to actually group the vectorized data points. \n",
    "\n",
    "PyTorch provides a built-in class called `DataLoader` for coordinating this process. \n",
    "\n",
    "We wrap the `DataLoader` inside the `generate_batches()` function, which is a generator to switch data between the CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_batches( dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    a generator function wrapping PyTorch DataLoader. \n",
    "    Ensures each tensor is on the write device location\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader( dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set the Namespace full of parameters for our workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_train_dataset_csv=\"data/yelp/raw_train.csv\",\n",
    "    raw_test_dataset_csv=\"data/yelp/raw_test.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.3,\n",
    "    output_munged_csv=\"data/yelp/reviews_with_splits_lite.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in the raw data into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = pd.read_csv(args.raw_train_dataset_csv, header=None, names=['rating', 'review'])\n",
    "# we select those entries where the review feature is not empty\n",
    "train_reviews = train_reviews[~pd.isnull(train_reviews.review)]\n",
    "test_reviews = pd.read_csv(args.raw_test_dataset_csv, header=None, names=['rating', 'review'])\n",
    "# we select those entries where the review feature is not empty\n",
    "test_reviews = test_reviews[~pd.isnull(test_reviews.review)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the first few entries of our different subsets of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       1  Unfortunately, the frustration of being Dr. Go...\n",
       "1       2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2       1  I don't know what Dr. Goldberg was like before...\n",
       "3       1  I'm writing this review to give you a heads up...\n",
       "4       2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ordered a large Mango-Pineapple smoothie. Stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quite a surprise!  \\n\\nMy wife and I loved thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>First I will say, this is a nice atmosphere an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>I was overall pretty impressed by this hotel. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Video link at bottom review. Worst service I h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       1  Ordered a large Mango-Pineapple smoothie. Stay...\n",
       "1       2  Quite a surprise!  \\n\\nMy wife and I loved thi...\n",
       "2       1  First I will say, this is a nice atmosphere an...\n",
       "3       2  I was overall pretty impressed by this hotel. ...\n",
       "4       1  Video link at bottom review. Worst service I h..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the number of each different kind of class in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    280000\n",
       "1    280000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simplistic method for preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text( text ):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews = pd.read_csv(args.output_munged_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply our custom preprocessing function by using `Series.apply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews.review = final_reviews.review.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    28000\n",
       "positive    28000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  terrible place to work for i just heard a stor...  train\n",
       "1  negative   hours , minutes total time for an extremely s...  train\n",
       "2  negative  my less than stellar review is for service . w...  train\n",
       "3  negative  i m granting one star because there s no way t...  train\n",
       "4  negative  the food here is mediocre at best . i went aft...  train"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Binary Classifier\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    simple perceptron-based classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int) - the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear( in_features=num_features , out_features=1 )\n",
    "    \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"\n",
    "        compute forward pass of classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor) - an input data tensor\n",
    "                x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool) - a flag for the sigmoid activation should be false if used with \n",
    "                cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. \n",
    "            tensor.shape should be (batch,)\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "training_args = Namespace( frequency_cutoff=25, \\\n",
    "                         model_state_file=\"model.pth\",\\\n",
    "                         review_csv=\"data/yelp/reviews_with_splits_lite.csv\",\\\n",
    "                         save_dir=\"data/yelp/\",\\\n",
    "                         vectorizer_file=\"vectorizer.json\",\\\n",
    "                         batch_size=128,\\\n",
    "                         early_stopping_criteria=5,\\\n",
    "                         learning_rate=0.001,\\\n",
    "                         num_epochs=100,\\\n",
    "                         seed=1337,\\\n",
    "                         cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': training_args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': training_args.model_state_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "***\n",
    "We instantiate the necessary objects to work with during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f86cd06476b4c668f5aabff5d8a9e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf7ffc623b64998a93c6f20149901ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=train', max=306, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679e4ecd25ce4a0e854f53844f191581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=val', max=65, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state = make_train_state(training_args)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    training_args.cuda = False\n",
    "training_args.device = torch.device(\"cuda\" if training_args.cuda else \"cpu\")\n",
    "\n",
    "# dataset and vectorizer\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(training_args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# model\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(training_args.device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode='min', factor=0.5,patience=1)\n",
    "\n",
    "# create status bars for the training progresses\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=training_args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(training_args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(training_args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(training_args.num_epochs):\n",
    "    train_state[\"epoch_index\"] = epoch_index\n",
    "    \n",
    "    dataset.set_split(\"train\")\n",
    "    batch_generator = generate_batches(dataset, batch_size=training_args.batch_size, device=training_args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # step 1: zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # step 2: compute the output\n",
    "        predictions = classifier(x_in=batch_dict[\"x_data\"].float())\n",
    "        # step 3: compute loss\n",
    "        loss = loss_func(predictions, batch_dict[\"y_target\"].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        # step 4: use loss to produce gradients\n",
    "        loss.backward()\n",
    "        # step 5: use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute accuracy\n",
    "        acc_batch = compute_accuracy(predictions, batch_dict[\"y_target\"])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "        \n",
    "        # update bar\n",
    "        train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "        \n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we iterate over the validation set\n",
    "dataset.set_split(\"val\")\n",
    "batch_generator = generate_batches(dataset, batch_size=training_args.batch_size, device=training_args.device)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # step 1: compute output\n",
    "    predictions = classifier(x_in=batch_dict[\"x_data\"].float())\n",
    "    # step 2: compute loss\n",
    "    loss = loss_func(predictions, batch_dict[\"y_target\"].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    \n",
    "train_state[\"val_loss\"].append(running_loss)\n",
    "train_state[\"val_acc\"].append(running_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can report our test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.321\n",
      "Test accuracy: 90.49\n"
     ]
    }
   ],
   "source": [
    "dataset.set_split(\"test\")\n",
    "batch_generator = generate_batches(dataset, batch_size=training_args.batch_size, device=training_args.device)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    predictions = classifier(x_in=batch_dict[\"x_data\"].float())\n",
    "    loss = loss_func(predictions, batch_dict[\"y_target\"].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    \n",
    "    acc_batch = compute_accuracy(predictions, batch_dict[\"y_target\"])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    \n",
    "train_state[\"test_loss\"] = running_loss\n",
    "train_state[\"test_acc\"] = running_acc\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(train_state[\"test_loss\"]))\n",
    "print(\"Test accuracy: {:.2f}\".format(train_state[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Inference on a New Unseen Example\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict_rating( review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"\n",
    "    predict rating of a new review\n",
    "    \n",
    "    Args:\n",
    "        review (str) - text of the review\n",
    "        classifier (ReviewClassifier) - trained model\n",
    "        vectorizer (ReviewVectorizer) - corresponding vectorizer\n",
    "        decision_threshold (float) - numerical boundary separating positive from negative ratings\n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1,-1))\n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "        \n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope that on the review \"this is a pretty awesome book\" will yield a \"positive\" class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "classifier = classifier.cpu() # move classifier code to cpu\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "print(\"{} -> {}\".format(test_review,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Weights Going into Decision Making\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential Words in Positive Reviews:\n",
      "--------------------------------------\n",
      "chinatown\n",
      "pleasantly\n",
      "mmmmmm\n",
      "deliciousness\n",
      "nclean\n",
      "eclectic\n",
      "hooked\n",
      "artsy\n",
      "amazed\n",
      "nexcellent\n",
      "heavenly\n",
      "spotless\n",
      "nhighly\n",
      "stunning\n",
      "keeper\n",
      "awesomeness\n",
      "chapel\n",
      "coma\n",
      "delectable\n",
      "mmmm\n"
     ]
    }
   ],
   "source": [
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "print(\"Influential Words in Positive Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential Words in Negative Reviews:\n",
      "--------------------------------------\n",
      "slowest\n",
      "cancelled\n",
      "unacceptable\n",
      "nmaybe\n",
      "underwhelmed\n",
      "operator\n",
      "worst\n",
      "meh\n",
      "subject\n",
      "canceled\n",
      "gossiping\n",
      "insulting\n",
      "blech\n",
      "mediocre\n",
      "horrendous\n",
      "awful\n",
      "embarrassing\n",
      "burden\n",
      "receipts\n",
      "inexcusable\n"
     ]
    }
   ],
   "source": [
    "print(\"Influential Words in Negative Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
