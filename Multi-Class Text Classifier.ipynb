{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is going to be to classify the question topic class of an observation.\n",
    "\n",
    "\n",
    "Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/question_topic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a look at 5 random observations from the dataset just to get a feel for what is looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question_text</th>\n",
       "      <th>question_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>423</td>\n",
       "      <td>Do you know if your app always has the same pr...</td>\n",
       "      <td>Omnichannel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>1123</td>\n",
       "      <td>What are the actual dimensions of the Peggy Mi...</td>\n",
       "      <td>Product Specifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>3988</td>\n",
       "      <td>I'm looking for a nice pair of silver stud ear...</td>\n",
       "      <td>Product Availability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>1873</td>\n",
       "      <td>I'm not sure I want to order the Komplete Meal...</td>\n",
       "      <td>Product Comparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>844</td>\n",
       "      <td>What's the weight capacity of the Spencer Wood...</td>\n",
       "      <td>Product Specifications</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      question_text  \\\n",
       "423          423  Do you know if your app always has the same pr...   \n",
       "1123        1123  What are the actual dimensions of the Peggy Mi...   \n",
       "3988        3988  I'm looking for a nice pair of silver stud ear...   \n",
       "1873        1873  I'm not sure I want to order the Komplete Meal...   \n",
       "844          844  What's the weight capacity of the Spencer Wood...   \n",
       "\n",
       "              question_topic  \n",
       "423              Omnichannel  \n",
       "1123  Product Specifications  \n",
       "3988    Product Availability  \n",
       "1873      Product Comparison  \n",
       "844   Product Specifications  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Specifications    839\n",
       "Product Availability      833\n",
       "Product Comparison        806\n",
       "Shipping                  799\n",
       "Returns & Refunds         768\n",
       "Sales/Promotions          505\n",
       "Omnichannel               450\n",
       "Name: question_topic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the classes are imbalanced because the smallest class is almost half of the largest class.\n",
    "\n",
    "We will write a function to preprocess the question text to make it more conducive to being used in a classification ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def clean_str( string ):\n",
    "    \"\"\"\n",
    "    Perform tokenization and cleaning for the strings\n",
    "    \"\"\"\n",
    "    # remove newline characters\n",
    "    string = re.sub(r\"\\n\",\"\",string)\n",
    "    # remove carriage return characters\n",
    "    string = re.sub(r\"\\r\",\"\",string)\n",
    "    # remove digits\n",
    "    string = re.sub(r\"[0-9]\",\"\",string)\n",
    "    # remove '\n",
    "    string = re.sub(r\"\\'\",\"\",string)\n",
    "    # remove \"\n",
    "    string = re.sub(r\"\\\"\",\"\",string)\n",
    "    # tokenzation and convert all text to lower case\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the cleaning to the question text column and create a new column for the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_question_text'] = data['question_text'].apply( clean_str )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pick out the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0               5000 non-null int64\n",
      "question_text            5000 non-null object\n",
      "question_topic           5000 non-null object\n",
      "cleaned_question_text    5000 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 156.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['question_text']\n",
    "y = data['question_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our inputs are the `question_text`, and are not numerical, we need to convert them into numerical features by performing a vectorization of the features. There are many different ways to perform featurization. We can use the `CountVectorizer` and the `TF-IDF`. We could also choose to include interaction terms.\n",
    "\n",
    "**CountVectorizer** - Transforms a review into a token count matrix. First it tokenizes the text and then creates a sparse matrix containing the count of occurrence of each word it detects.\n",
    "\n",
    "**TF-IDF** - represents the importance of a word to a document in a corpus. The TF-IDF value is proportional to the frequency of a word in a document. The TF-IDF is computed as the term frequency (TF) times the inverse document frequency (IDF). If a review contains 100 words, and 5 of them are the word Awesome, then the term frequency for the word Awesome is (5/100)=0.05. If there are then 1 million reviews in the entire corpus and the word Awesome appears 1000 times in the entire corpus, then the inverse document frequency is computed as $log(1000000/1000) =  3$. THen the TF-IDF value is $0.05 * 3=0.15$\n",
    "\n",
    "\n",
    "Now we must choose the multi-class model. Here we'll use MultiClass SVM in the one versus all paradigm. In this case, there is one binary classifier trained for each class, and it is trained to identify the presence of its corresponding class. Passing the `class_weight=\"balanced\"` means that the classifier will try to remove the biasedness of the model due to the imbalance of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipeline = Pipeline( [('vectorizer', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', OneVsRestClassifier( LinearSVC(class_weight=\"balanced\")))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fine tune the model by performing hyperparameter selection. We pass the classifier we're using, as well as grid specifying the values for the hyperparameters to try. We pass the `GridSearchCV` the `n_jobs=-1` to tell it to use all cores, and `cv=5` to do 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ..._class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)], 'tfidf__use_idf': (True, False)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = { 'vectorizer__ngram_range': [(1,1),(1,2),(2,2)],\n",
    "              'tfidf__use_idf': (True, False) }\n",
    "svm_clf_grid = GridSearchCV( pipeline, parameters, n_jobs=-1, cv=5 )\n",
    "svm_clf_grid.fit( X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9672\n",
      "Best Set of parameters: {'vectorizer__ngram_range': (1, 2), 'tfidf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: {}\".format(svm_clf_grid.best_score_))\n",
    "print(\"Best Set of parameters: {}\".format(svm_clf_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the model with the training data and the evaluate on the test data to get an idea of the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline( [('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     ('clf', OneVsRestClassifier( LinearSVC(class_weight=\"balanced\")))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           Omnichannel       1.00      1.00      1.00       131\n",
      "  Product Availability       0.99      0.99      0.99       236\n",
      "    Product Comparison       0.98      0.99      0.99       247\n",
      "Product Specifications       0.98      0.98      0.98       253\n",
      "     Returns & Refunds       1.00      1.00      1.00       248\n",
      "      Sales/Promotions       1.00      0.99      0.99       152\n",
      "              Shipping       1.00      1.00      1.00       233\n",
      "\n",
      "             micro avg       0.99      0.99      0.99      1500\n",
      "             macro avg       0.99      0.99      0.99      1500\n",
      "          weighted avg       0.99      0.99      0.99      1500\n",
      "\n",
      "[[131   0   0   0   0   0   0]\n",
      " [  0 233   1   2   0   0   0]\n",
      " [  0   0 245   2   0   0   0]\n",
      " [  0   1   4 248   0   0   0]\n",
      " [  0   0   0   0 248   0   0]\n",
      " [  0   1   0   1   0 150   0]\n",
      " [  0   0   0   0   0   0 233]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "model.fit( X_train, y_train )\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there are many values for the precision, recall and the f1-score because we used a OneVsAll model. In this model we trained a binary classifier for every class.\n",
    "\n",
    "For further steps and better accuracy, we could try the following:\n",
    "\n",
    "* Eliminate low quality features (words)\n",
    "* Remove stopwords\n",
    "* Diversify the training corpus\n",
    "* Use stemming and Lemmatizing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
