{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we normalize and reduce the number of common words into a single word for text analysis?\n",
    "\n",
    "These two techniques are used in SEO, tagging, and indexing systems.\n",
    "\n",
    "# Stemming\n",
    "***\n",
    "**Stemming is about stripping suffixes.**\n",
    "\n",
    "Some words can be reduced to a single word, so stemming involves removing the last characters of a word until we can get a common word to represent a number of words. The final word is known as the _lemma_. \n",
    "\n",
    "An example of this would be that the words \"blogging\", \"blogged\", and \"blogs\" can all be reduced to the single root word word \"blog\". \n",
    "\n",
    "Some of the most common stemming algorithms are Porter, Lancaster and Snowball. \n",
    "\n",
    "Here is an example of using the snowball stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog\n",
      "blog\n",
      "blog\n"
     ]
    }
   ],
   "source": [
    "from nltk import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "for word in ['blogging','blogged','blogs']:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, those three words have the same root word, and so when we perform stemming, the suffixes that contain no new information are removed to reveal the similarity between the words.\n",
    "\n",
    "# Lemmatisation\n",
    "***\n",
    "Lemmatisation is the process of reducing the number of words into a single word by combining common words together. This is similar to transforming to the dictionary base form.\n",
    "\n",
    "Here is an example of using the lemmatizer built into `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"blogs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "***\n",
    "Both of these techniques are useful for reducing the number of common words into a single word. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
