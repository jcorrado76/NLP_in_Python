{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will clearly explain how to use the following algorithms:\n",
    "\n",
    "1. N-Grams\n",
    "2. Bag of Words\n",
    "3. Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "***\n",
    "N-Grams are sets of consecutive works. N-Grams allow us to predict the next words of a piece of text.\n",
    "\n",
    "Consider the sentence \"FinTechExplained is a publication\". \n",
    "\n",
    "1-Grams: FinTechExplained,is,a,publication\n",
    "\n",
    "2-Grams: FinTechExplained is, is a, a publication\n",
    "\n",
    "3-Grams: FinTechExplained is a, is a publication\n",
    "\n",
    "We can compute the N-grams using `NLTK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One grams:\n",
      "('FinTechExplained',)\n",
      "('is',)\n",
      "('a',)\n",
      "('publication',)\n",
      "\n",
      "Two grams:\n",
      "('FinTechExplained', 'is')\n",
      "('is', 'a')\n",
      "('a', 'publication')\n",
      "\n",
      "Three grams:\n",
      "('FinTechExplained', 'is', 'a')\n",
      "('is', 'a', 'publication')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = 'FinTechExplained is a publication'\n",
    "tokenized_text = nltk.word_tokenize( text )\n",
    "\n",
    "\n",
    "one_grams = ngrams( tokenized_text, 1 )\n",
    "two_grams = ngrams( tokenized_text, 2 )\n",
    "three_grams = ngrams( tokenized_text, 3 )\n",
    "\n",
    "print(\"One grams:\")\n",
    "for one_gram in one_grams:\n",
    "    print(one_gram)\n",
    "print(\"\\nTwo grams:\")\n",
    "for two_gram in two_grams:\n",
    "    print(two_gram)\n",
    "print(\"\\nThree grams:\")\n",
    "for three_gram in three_grams:\n",
    "    print(three_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "***\n",
    "We need a numerical representation for our words before it can be applied into a machine learning algorithm.\n",
    "\n",
    "One approach is to count the occurrence of words in a document. The bag of words representation is about creating a matrix of words, where the words are represented as rows and the columns represent the document names. Then we can populate this matrix with the frequency of each term within the document.\n",
    "\n",
    "This matrix is called the **Term Document Matrix**.\n",
    "\n",
    "**Each row is a word vector. Each column is a document vector**\n",
    "\n",
    "One example is if we consider a set of tweets from twitter and statuses from facebook, containing the word \"NLP\". \n",
    "\n",
    "One may tokenize the sentences into words and then populate the TDM where there will be a column for Facebook and then another column for Twitter.\n",
    "\n",
    "The rows will correspond to the words occurring in any of the tweets or posts. The values of this matrix will correspond to the count of that word occurring in the document corresponding to that column.\n",
    "\n",
    "We will show how to populate this dictionary using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "twitter_data = []\n",
    "facebook_data = []\n",
    "\n",
    "data = {'twitter':twitter_data,\\\n",
    "       'facebook':facebook_data}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "fitted_vectorizer = vectorizer.fit_transform( data['twitter']\\\n",
    "                    .append(data['facebook']) )\n",
    "\n",
    "TDM = pd.DataFrame( fitted_vectorizer.toarray().transpose(),\\\n",
    "                  index=fitted_vectorizer.get_feature_names(),\\\n",
    "                 columns=['twitter','facebook'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "***\n",
    "TF-IDF is a great statistical measure for understanding the relevance of the word within a corpus of documents.\n",
    "\n",
    "For each document, we compute a matrix by performing the steps:\n",
    "* TF - (term frequency) calculate the number of times a term appears in a document divided by total number of terms in a document\n",
    "* IDF - (inverse document frequency) compute the logarithm of the total number of documents, divided by the number of documents containing the term.\n",
    "* Multiply the TF by the IDF. \n",
    "\n",
    "Rows of this matrix represent terms and the columns represent document names.\n",
    "\n",
    "## A Simple Example\n",
    "***\n",
    "Assume there are 100 documents, and 4 of them contain the term \"FinTechExplained\" (mentioned once in the first and second document, twice in the third document and three times in the fourth document). Also assume there are 100 words in each document.\n",
    "\n",
    "There are 100 words in each of the 100 documents. Only four of the documents contain the word of interest.\n",
    "\n",
    "In this case, the TF in each of the four documents will be:\n",
    "\n",
    "Document 1 - 1/100\n",
    "\n",
    "Document 2 - 1/100\n",
    "\n",
    "Document 3 - 2/100\n",
    "\n",
    "Document 4 - 3/100\n",
    "\n",
    "For the inverse document frequency, because four of the 100 documents contain the word of interest, we get $\\log(100/4)$ for the IDF.\n",
    "\n",
    "Therefore, the TF-IDF of the term of interest in Document 1 is:\n",
    "$(1/100)(\\log(100/4))$\n",
    "\n",
    "We can compute the same quantity using scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "fitted_vectorizer = vectorizer.fit_transform( data['twitter']\\\n",
    "                    .append(data['facebook']) )\n",
    "\n",
    "dataframe = pd.DataFrame( fitted_vectorizer.toarray().transpose(), \\\n",
    "                    index=fitted_vectorizer.get_feature_names(),\\\n",
    "                    columns=['twitter','facebook'] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
