{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-19 23:52:27,596 : INFO : collecting all words and their counts\n",
      "2019-08-19 23:52:27,597 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-08-19 23:52:27,597 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2019-08-19 23:52:27,598 : INFO : Loading a fresh vocabulary\n",
      "2019-08-19 23:52:27,599 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2019-08-19 23:52:27,599 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2019-08-19 23:52:27,600 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2019-08-19 23:52:27,600 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2019-08-19 23:52:27,601 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2019-08-19 23:52:27,601 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2019-08-19 23:52:27,602 : INFO : resetting layer weights\n",
      "2019-08-19 23:52:27,602 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-08-19 23:52:27,605 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-19 23:52:27,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-19 23:52:27,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-19 23:52:27,606 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-08-19 23:52:27,608 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-19 23:52:27,609 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-19 23:52:27,610 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-19 23:52:27,610 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-08-19 23:52:27,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-19 23:52:27,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-19 23:52:27,613 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-19 23:52:27,614 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 572 effective words/s\n",
      "2019-08-19 23:52:27,617 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-19 23:52:27,617 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-19 23:52:27,618 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-19 23:52:27,618 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-08-19 23:52:27,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-08-19 23:52:27,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-08-19 23:52:27,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-08-19 23:52:27,621 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-08-19 23:52:27,622 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 52 effective words/s\n",
      "2019-08-19 23:52:27,622 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [[\"first\", \"sentence\"], [\"second\", \"sentence\"]]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim requires that we provide input sentences sequentially, to be iterated over. \n",
    "\n",
    "If the input is held across several files on disk, with one sentence per line, then we can process the input file by file, line by line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    def __iter__(self):\n",
    "        for fileName in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "sentences = MySentences(\"/some/directory\")\n",
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to perform further preprocessing, it can be encapsulated inside the `__iter__` method. The method must yield a single sentence (a list of utf-8 encoded words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for training.\n",
    "\n",
    "`min_count` - Prune the internal dictionary in order to make words that only appear once or twice not be kept track of. \n",
    "\n",
    "`size` - the size of the NN layers. \n",
    "\n",
    "`workers` - for training parallelization\n",
    "\n",
    "Save your model with:\n",
    "\n",
    "`model.save(\"/tmp/mymodel\")`\n",
    "\n",
    "And load it with:\n",
    "\n",
    "`new_model = gensim.models.Word2Vec.load(\"/tmp/mymodel\")`\n",
    "\n",
    "You can load a model and then continue training it with more sentences:\n",
    "\n",
    "`new_model = gensim.models.Word2Vec.load(\"/tmp/mymodel\")\n",
    "model.train(more_sentences)`\n",
    "\n",
    "You can get the raw output vectors with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"computer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or compute similarity between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"woman\",\"man\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
