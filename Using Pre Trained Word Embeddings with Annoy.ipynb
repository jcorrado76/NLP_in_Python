{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to be using the 100 dimensional GloVe embeddings from Stanford. I'm just going to show how to get comfortable around the usage of this class for reading in word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     87
    ]
   },
   "outputs": [],
   "source": [
    "class PreTrainedEmbeddings(object):\n",
    "    def __init__(self, word_to_index, word_vectors=None, annoy_index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_to_index (dict): mapping from word tokens to integers\n",
    "            word_vectors (list): list of numpy arrays\n",
    "        \"\"\"\n",
    "        if word_vectors is None and annoy_index is None:\n",
    "            # failed to pass either manual word vectors, or an already made AnnoyIndex\n",
    "            assert RuntimeError, \"must specify either word_vectors or annoy_index\"\n",
    "        elif word_vectors is not None:\n",
    "            # you've passed in a list of word vectors\n",
    "            self.word_to_index = word_to_index\n",
    "            self.word_vectors = word_vectors\n",
    "            self.index_to_word = {idx: word for word,\n",
    "                                  idx in self.word_to_index.items()}\n",
    "            embedding_dimension = len(word_vectors[0])\n",
    "            self.index = AnnoyIndex(embedding_dimension, metric=\"euclidean\")\n",
    "            for _, i in self.word_to_index.items():\n",
    "                self.index.add_item(i, self.word_vectors[i])\n",
    "            self.index.build(50)\n",
    "        else:\n",
    "            # loading word vectors and annoy index from file\n",
    "            self.word_to_index = word_to_index\n",
    "            self.index = annoy_index\n",
    "            self.index_to_word = {idx: word for word,\n",
    "                                  idx in self.word_to_index.items()}\n",
    "            \n",
    "    @classmethod\n",
    "    def from_embeddings_file(cls, embedding_file):\n",
    "        \"\"\"\n",
    "        Instantiate PreTrainedEmbeddings instance from a pretrained vector file\n",
    "\n",
    "        Assumes the pretrained vector file is of the format:\n",
    "            word0 x0_0 x0_1 ... x0_N\n",
    "            word1 x1_0 x1_1 ... x1_N\n",
    "            ...\n",
    "\n",
    "        Args:\n",
    "            embedding_file (str): location of the file\n",
    "        Returns:\n",
    "            PreTrainedEmbeddings instance\n",
    "        \"\"\"\n",
    "        # give a word and return the index\n",
    "        word_to_index = {}\n",
    "        word_vectors = []\n",
    "        with open(embedding_file) as fp:\n",
    "            for line in fp.readlines():\n",
    "                line = line.split(\" \")\n",
    "                # the token is always the first column\n",
    "                word = line[0]\n",
    "                # the word vector is the rest of the columns\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "        return cls(word_to_index, word_vectors)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_annoy_mmap(cls, mmap_path, word_to_index_path):\n",
    "        with open(word_to_index_path, 'rb') as word_to_index_file_obj:\n",
    "            word_to_index_dict = pickle.load(word_to_index_file_obj)\n",
    "        annoy_index = AnnoyIndex(100, metric=\"euclidean\")\n",
    "        annoy_index.load(mmap_path)\n",
    "        return cls(word_to_index_dict, word_vectors=None, annoy_index = annoy_index )\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Return the word vector corresponding to word\n",
    "\n",
    "        Args:\n",
    "            word (str)\n",
    "        Returns:\n",
    "            embedding (np.ndarray)\n",
    "        \"\"\"\n",
    "        word_index = self.word_to_index[word]\n",
    "        return np.array(self.index.get_item_vector(word_index))\n",
    "\n",
    "    def get_closest_to_vector(self, target_word_vector, n=1):\n",
    "        \"\"\"\n",
    "        Given a target word vector, return its n nearest neighbors in the vocabulary\n",
    "\n",
    "        Args:\n",
    "            target_word_vector (np.ndarray): needs to match the size of vectors in AnnoyIndex\n",
    "        Returns:\n",
    "            [str, str, ...]: list of word tokens nearest to target word vector\n",
    "        \"\"\"\n",
    "        # get the indices for the nearest neighbors\n",
    "        nearest_neighbor_indices = self.index.get_nns_by_vector(\n",
    "            target_word_vector, n)\n",
    "        # find their corresponding word vectors\n",
    "        return [self.index_to_word[neighbor] for neighbor in nearest_neighbor_indices]\n",
    "\n",
    "    def compute_and_print_analogy(self, word1, word2, word3, relationship=\"is to\"):\n",
    "        \"\"\"\n",
    "        Prints solution for word4 in the analogy:\n",
    "        word1 is to word2 as word3 is to ___\n",
    "\n",
    "        Args:\n",
    "            word1 (str)\n",
    "            word2 (str)\n",
    "            word3 (str)            \n",
    "        \"\"\"\n",
    "        vec1 = self.get_embedding(word1)\n",
    "        vec2 = self.get_embedding(word2)\n",
    "        vec3 = self.get_embedding(word3)\n",
    "\n",
    "        spatial_relationship = vec2 - vec1\n",
    "        vec4 = vec3 + spatial_relationship\n",
    "\n",
    "        closest_words = self.get_closest_to_vector(vec4, n=4)\n",
    "        existing_words = set([word1, word2, word3])\n",
    "        closest_words = [\n",
    "            word for word in closest_words if word not in existing_words]\n",
    "\n",
    "        if len(closest_words) == 0:\n",
    "            print(\"Could not find any nearest neighbors for target vector!\")\n",
    "            return\n",
    "        for word4 in closest_words:\n",
    "            print(\"{} {} {} as {} {} {}\".format(\n",
    "                word1, relationship, word2, word3, relationship, word4))\n",
    "\n",
    "    def save_embeddings(self, pickle_out_path, mmap_out_path):\n",
    "        # save the annoy index\n",
    "        self.index.save(mmap_out_path)\n",
    "        # save the word_to_index dictionary as pickle\n",
    "        with open(pickle_out_path, 'wb') as word_to_index_outfile:\n",
    "            pickle.dump(self.word_to_index, word_to_index_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"data/glove_wikipedia_embeddings/\"\n",
    "annoy_mmap_out_file_name = \"glove.6B.100d.txt_annoy\"\n",
    "pickle_out_filename = \"glove.6B.100d_word_to_index_dict.pkl\"\n",
    "# output location for annoy mmap\n",
    "index_out_path = os.path.join(out_dir,annoy_mmap_out_file_name)\n",
    "# output location for the word_to_index annoy mmap\n",
    "pickle_out_path = os.path.join(out_dir, pickle_out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy mmap and word to index file already exist. Loading from disk..\n",
      "CPU times: user 131 ms, sys: 54.2 ms, total: 186 ms\n",
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make sure the annoy index mmap and the word to index dictionary pickle file both exist\n",
    "if os.path.exists(index_out_path) and os.path.exists(pickle_out_path):\n",
    "    print(\"Annoy mmap and word to index file already exist. Loading from disk..\")\n",
    "    embeddings = PreTrainedEmbeddings.from_annoy_mmap(mmap_path=index_out_path, \n",
    "        word_to_index_path=pickle_out_path)\n",
    "else:\n",
    "    embeddings_file_path = \"data/glove_wikipedia_embeddings/glove.6B.100d.txt\"\n",
    "    print(\"Creating new embeddings from file: {}\".format(embeddings_file_path))\n",
    "    embeddings = PreTrainedEmbeddings.from_embeddings_file(embeddings_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our annoy index object wrapper so we don't have to regenerate the annoy index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.save_embeddings(pickle_out_path=pickle_out_path, mmap_out_path=index_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relationship between gendered nouns and pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man uses pronoun he as woman uses pronoun she\n",
      "man uses pronoun he as woman uses pronoun never\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"man\",\"he\",\"woman\", relationship=\"uses pronoun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship between verb noun relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly is to plane as sail is to ship\n",
      "fly is to plane as sail is to vessel\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"fly\",\"plane\",\"sail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship between noun-noun relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat is an older kitten as dog is an older puppy\n",
      "cat is an older kitten as dog is an older toddler\n",
      "cat is an older kitten as dog is an older sleds\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"cat\",\"kitten\",\"dog\", relationship=\"is an older\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypernymy relationship - a word is a member of a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue is a kind of color as dog is a kind of cat\n",
      "blue is a kind of color as dog is a kind of animal\n",
      "blue is a kind of color as dog is a kind of breed\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"blue\",\"color\",\"dog\", relationship=\"is a kind of\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meronymy - a word to a part of a whole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toe is a part of the foot as finger is a part of the hand\n",
      "toe is a part of the foot as finger is a part of the attached\n",
      "toe is a part of the foot as finger is a part of the apart\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"toe\",\"foot\",\"finger\", relationship=\"is a part of the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troponymy - difference in manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk is how you communicate as read is how you instructions\n",
      "talk is how you communicate as read is how you communicating\n",
      "talk is how you communicate as read is how you transmit\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"talk\",\"communicate\",\"read\",relationship=\"is how you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metonymy - convention / figure of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue is the color for democrat as red is the color for republican\n",
      "blue is the color for democrat as red is the color for congressman\n",
      "blue is the color for democrat as red is the color for senator\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"blue\",\"democrat\",\"red\",relationship=\"is the color for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjectival scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast is the fastest as young is the youngest\n",
      "fast is the fastest as young is the female\n",
      "fast is the fastest as young is the younger\n",
      "fast is the fastest as young is the sixth\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"fast\",\"fastest\",\"young\",relationship=\"is the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the canonical example of word embeddings! However, you must watch out for things such as gender being encoded in word embeddings as this can introduce unwanted biases in downstream models (remember! humans created the text data our models are trained on!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to king as woman is to queen\n",
      "man is to king as woman is to monarch\n",
      "man is to king as woman is to throne\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"man\",\"king\",\"woman\",relationship=\"is to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating between language regularities and codified cultural biases is difficult. For example, long standing biases in culture lead our word embedding to learn that men are most frequently associated with doctors, whereas women are most often associated with nurses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to doctor as woman is to nurse\n",
      "man is to doctor as woman is to physician\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"man\",\"doctor\",\"woman\",relationship=\"is to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from any real sociological concerns, realize that these undercurrent biases lead our model to learn an objectively inaccurate model for the English language (i.e. the word doctor is not defined as being only applicable to men-in the language our model was trained on, doctors were just most often associated with men)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
