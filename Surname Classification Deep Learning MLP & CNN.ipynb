{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Objective\n",
    "***\n",
    "Our objective in this project is to predict the nationality associated with a given last name. \n",
    "\n",
    "The original dataset contains 10000 surnames from 18 different nationalities. This dataset is imbalanced, as there is larger representation from certain nationalities than that of others. \n",
    "\n",
    "Here are some imports needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(asctime)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Two_Layer_MLP(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int) - size of input vector\n",
    "            hidden_dim (int) - the size after first Linear Layer\n",
    "            output_dim (int) - size after second Linear Layer\n",
    "        \"\"\"\n",
    "        super(Two_Layer_MLP, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        print(\"Initialized a two-layer MLP\")\n",
    "        print(\"Number of features: {}\".format(num_features))\n",
    "        print(\"Number of hidden units: {}\".format(hidden_dim))\n",
    "        print(\"Number of output classes: {}\".format(output_dim))\n",
    "        print(\"Dropout Probability: {}\".format(self.dropout_prob))\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        compute forward pass\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor) - input data tensor. x_in.shape is (batch, num_features)\n",
    "            apply_softmax (bool) - a flag for the softmax activation. \n",
    "                                    should be False if used with cross-entropy loss\n",
    "        Returns:\n",
    "            resulting tensor. tensor.shape is (batch, output_dim)\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in)) # output of first hidden layer\n",
    "        output = self.fc2(F.dropout(intermediate,p=self.dropout_prob))       # final output vector\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to instantiate an MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2   # 2 rows at once\n",
    "num_features  = 3   # 3 original features\n",
    "hidden_dim = 100 # 100 nodes in first hidden layer\n",
    "output_dim = 4   # output 4-d vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a two-layer MLP\n",
      "Number of features: 3\n",
      "Number of hidden units: 100\n",
      "Number of output classes: 4\n",
      "Dropout Probability: 0.5\n",
      "Two_Layer_MLP(\n",
      "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#initialize the Model\n",
    "mlp = Two_Layer_MLP(num_features, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the integrity to make sure we got the dimensions correctly by passing some random inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.7956, 0.3826, 0.8064],\n",
      "        [0.8380, 0.2642, 0.3103]])\n",
      "\n",
      "Output vectors:\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 4])\n",
      "Values: \n",
      "tensor([[ 0.2634, -0.1211,  0.3391, -0.0751],\n",
      "        [ 0.0641,  0.4682,  0.1981, -0.0709]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "def describe(tensor):\n",
    "    \"\"\"\n",
    "    Given a PyTorch tensor, print some of its properties\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): input tensor to describe\n",
    "    \"\"\"\n",
    "    print(\"Type: {}\".format(tensor.type()))\n",
    "    print(\"Shape/size: {}\".format(tensor.shape))\n",
    "    print(\"Values: \\n{}\".format(tensor))\n",
    "\n",
    "x_input = torch.rand(size=(batch_size, num_features))\n",
    "print(\"Input data:\")\n",
    "describe(x_input)\n",
    "print()\n",
    "y_output = mlp.forward(x_input, apply_softmax=False) # you may omit the 'forward' here\n",
    "print(\"Output vectors:\")\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can convert each of the output vectors (each row) into a vector of probabilities by enabling the softmax activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vectors after softmax:\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 4])\n",
      "Values: \n",
      "tensor([[0.2054, 0.2318, 0.2708, 0.2920],\n",
      "        [0.2200, 0.1976, 0.3151, 0.2673]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_output_softmax = mlp(x_input, apply_softmax=True)\n",
    "print(\"Output vectors after softmax:\")\n",
    "describe(y_output_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to make sure that each of these 2 rows sums to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999999, 0.9999999], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output_softmax.detach().numpy().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_num_channels (int): size of input feature vector\n",
    "            num_classes (int): size of output prediction vector\n",
    "            num_channels (int): constant channel size to use throughout network\n",
    "        \"\"\"\n",
    "        super(CNN_Model, self).__init__()\n",
    "        print(\"Initialized CNN\")\n",
    "        self.convnet = \\\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=initial_num_channels, out_channels=num_channels, kernel_size=3),\n",
    "                nn.ELU(),\n",
    "                nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, stride=2),\n",
    "                nn.ELU(),\n",
    "                nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, stride=2),\n",
    "                nn.ELU(),\n",
    "                nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=3),\n",
    "                nn.ELU()\n",
    "            )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "    def forward(self, x_surname, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_surname (torch.Tensor): input data tensor,\n",
    "                                      of size (batch, initial_num_channels, max_surname_length)\n",
    "            apply_softmax (bool): whether to use softmax activation\n",
    "                                  should be false when using Cross Entropy loss\n",
    "        \"\"\"\n",
    "        features = self.convnet(x_surname).squeeze(dim=2)\n",
    "        predictions = self.fc(features)\n",
    "        if apply_softmax:\n",
    "            predictions = F.softmax(predictions, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Vectorization Classes\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     4,
     29,
     38,
     43,
     60,
     68,
     83,
     95,
     97
    ]
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Vocabulary object manages the dictionary of tokens to indexes and the dictionary of indexes to tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag indicating whether to add UNK token\n",
    "            unk_token (str): the UNK tokne to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        # if the token->idx dictionary does not exist\n",
    "        if token_to_idx is None:\n",
    "            # create it\n",
    "            token_to_idx = {}\n",
    "        \n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        # whether to add a token for unknown words\n",
    "        self._add_unk = add_unk\n",
    "        # the token to use for unknown words\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        # if adding token for unknown words\n",
    "        if add_unk:\n",
    "            # index to use for unknown words\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "        else:\n",
    "            self.unk_index = -1\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        this method returns a dictionary containing the token to index dictionary, \n",
    "        whether or not we added an UNK token, and the token used\n",
    "        \"\"\"\n",
    "        return {\"token_to_idx\": self._token_to_idx,\n",
    "               \"add_unk\": self._add_unk,\n",
    "               \"unk_token\": self._unk_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        instantiate a Vocabulary instance from a dictionary\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Update the two dictionaries, adding the newest token\n",
    "        \n",
    "        Args:\n",
    "            token (str): token to add to dictionary\n",
    "        Returns:\n",
    "            index (int): integer corresponding to token\n",
    "        \"\"\"\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            # get length of current list to be the index of new one \n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    def get_index(self, token):\n",
    "        \"\"\"\n",
    "        Get index from token.\n",
    "        If unk_index >= 0 then it has been added into vocabulary to enabled the UNK functionality\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token whose index to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to token\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            # None safe getitem; return self.unk_index if not found\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def get_token(self, index):\n",
    "        \"\"\"\n",
    "        Get token associated to index.\n",
    "        \n",
    "        Args:\n",
    "            index (int): index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to index\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"Index {0:d} not in Vocabulary\".format(index))\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={0:d})>\".format(len(self))\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     1,
     4,
     17,
     31
    ]
   },
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname (str): the surname to vectorize\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): a collapsed one-hot encoding\n",
    "        \"\"\"\n",
    "        # initialize empty one-hot representation\n",
    "        one_hot_vec = np.zeros(shape=(len(self.surname_vocab),), dtype=np.float32)\n",
    "        for token in surname:\n",
    "            one_hot_vec[self.surname_vocab.get_index(token)] = 1\n",
    "        return one_hot_vec\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"\n",
    "        create an instance of vectorizer from a dataframe of surnames.\n",
    "        create the constituent vocabularies and fill them from scratch\n",
    "        \"\"\"\n",
    "        # initialize empty vocab using @ as unk token\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Initialize a surname vectorizer from a dictionary containing vocabularies\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary.from_serializable(contents[\"surname_vocab\"])\n",
    "        nationality_vocab = Vocabulary.from_serializable(contents[\"nationality_vocab\"])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        create a dictionary containing the surname and nationality dictionaries\n",
    "        \"\"\"\n",
    "        return {\"surname_vocab\": self.surname_vocab.to_serializable(),\n",
    "               \"nationality_vocab\": self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     1,
     21,
     37,
     45
    ]
   },
   "outputs": [],
   "source": [
    "class CNN_SurnameVectorizer(SurnameVectorizer):\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
    "        SurnameVectorizer.__init__(self, surname_vocab, nationality_vocab)\n",
    "        self._max_surname_length = max_surname_length\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname (str): the surname to vectorize\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): a collapsed one-hot encoding\n",
    "        \"\"\"\n",
    "        # initialize empty one-hot representation\n",
    "        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)\n",
    "        one_hot_matrix = np.zeros(shape=one_hot_matrix_size, dtype=np.float32)\n",
    "        \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.get_index(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"\n",
    "        create an instance of vectorizer from a dataframe of surnames.\n",
    "        create the constituent vocabularies and fill them from scratch\n",
    "        \"\"\"\n",
    "        # initialize empty vocab using @ as unk token\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        max_surname_length = 0\n",
    "        for index, row in surname_df.iterrows():\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Initialize a surname vectorizer from a dictionary containing vocabularies\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary.from_serializable(contents[\"surname_vocab\"])\n",
    "        nationality_vocab = Vocabulary.from_serializable(contents[\"nationality_vocab\"])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab, \n",
    "                  max_surname_length=contents['max_surname_length'])\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        create a dictionary containing the surname and nationality dictionaries\n",
    "        \"\"\"\n",
    "        return {\"surname_vocab\": self.surname_vocab.to_serializable(),\n",
    "               \"nationality_vocab\": self.nationality_vocab.to_serializable(),\n",
    "               \"max_surname_length\":self._max_surname_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     1,
     27,
     40,
     45,
     56,
     59,
     61,
     64,
     66,
     75
    ]
   },
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.surname_df[self.surname_df[\"split\"] == \"train\"]\n",
    "        self.train_size= len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.surname_df[self.surname_df[\"split\"] == \"val\"]\n",
    "        self.validation_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.surname_df[self.surname_df[\"split\"] == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._data_dictionary = {\"train\": (self.train_df, self.train_size),\n",
    "                                \"val\": (self.val_df, self.validation_size),\n",
    "                                \"test\": (self.test_df, self.test_size)}\n",
    "        self.set_split(\"train\")\n",
    "        print(\"Initialized dataset split to train\")\n",
    "        \n",
    "        class_counts = surname_df[\"nationality\"].value_counts().to_dict()\n",
    "        # sort the counts by its index in nationality dictionary\n",
    "        sorted_counts = sorted(class_counts.items(), \n",
    "            key=lambda item: self._vectorizer.nationality_vocab.get_index(item[0]))\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"\n",
    "        Load a dataset from file and create a vectorizer from the training portion of it \n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df[\"split\"] == \"train\"]\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        load only vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): location of serialized vectorizer\n",
    "        Returns:\n",
    "            instance of SurnameVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fileHandle:\n",
    "            return SurnameVectorizer.from_serializable( json.load(fileHandle) )\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as outFile:\n",
    "            json.dump(self._vectorizer.to_serializable(), outFile)\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._current_split = split\n",
    "        self._current_df, self._current_size = self._data_dictionary[split]\n",
    "    def __len__(self):\n",
    "        return self._current_size\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        entryoint for PyTorch Dataset\n",
    "        \"\"\"\n",
    "        row = self._current_df.iloc[index]\n",
    "        surname_vector = self._vectorizer.vectorize(row.surname)\n",
    "        nationality_index = self._vectorizer.nationality_vocab.get_index(row.nationality)\n",
    "        return {\"x_surname\": surname_vector, \n",
    "               \"y_nationality\": nationality_index}\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        using currently active split, compute how many batches are in this set\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     7,
     10,
     23
    ]
   },
   "outputs": [],
   "source": [
    "class CNN_SurnameDataset(SurnameDataset):\n",
    "    \"\"\"\n",
    "    the input to the CNN model, rather than the collapsed one hot vectors, needs to be a matrix of one hot\n",
    "    vectors.\n",
    "    we also need to change the __getitem__ method to track the longest surname and ensure we pass it to the \n",
    "    vectorizer so we know how many rows to include in the matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        SurnameDataset.__init__(self, surname_df, vectorizer)\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"\n",
    "        Load a dataset from file and create a vectorizer from the training portion of it \n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df[\"split\"] == \"train\"]\n",
    "        return cls(surname_df, CNN_SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        load only vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): location of serialized vectorizer\n",
    "        Returns:\n",
    "            instance of SurnameVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fileHandle:\n",
    "            return CNN_SurnameVectorizer.from_serializable( json.load(fileHandle) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_batches( dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\" ):\n",
    "    \"\"\"\n",
    "    a wrapper for PyTorch DataLoader that exposes a generator for lazy iteration.\n",
    "    before consuming, ensures tensor is located on correct device\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Parameters\n",
    "***\n",
    "Here are some helper functions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     16,
     48,
     53,
     58
    ]
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    \"\"\"\n",
    "    yields a dictionary created from values of args\n",
    "    \"\"\"\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"\n",
    "    Update the training state.\n",
    "    Implements early stopping to prevent overfitting\n",
    "    Implements Model Checkpoitns to only save model if it is better\n",
    "    \"\"\"\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "def compute_accuracy(predictions, true_values):\n",
    "    # on output vectors, pick column with highest prob\n",
    "    _, prediction_indices = predictions.max(dim=1)\n",
    "    n_correct = torch.eq(prediction_indices, true_values).sum().item()\n",
    "    return n_correct / len(prediction_indices) * 100\n",
    "def set_seed_everywhere( seed, cuda ):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some args that we want to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_file = \"data/surnames_with_splits.csv\",\n",
    "    vectorizer_file = \"surname_vectorizer.json\",\n",
    "    model_state_file = \"model.pth\",\n",
    "    save_dir = \"data\",\n",
    "    hidden_dim=300,\n",
    "    seed=2019,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    cuda=False,\n",
    "    reload_from_files=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer file full path:\n",
      "data/surname_vectorizer.json\n",
      "Model state file full path:\n",
      "data/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "# prepend save directory to filenames\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "print(\"Vectorizer file full path:\\n{}\".format(args.vectorizer_file))\n",
    "print(\"Model state file full path:\\n{}\".format(args.model_state_file))\n",
    "\n",
    "# check to see if we can use CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "# set seed everywhere for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating a new vectorizer\n",
      "Initialized dataset split to train\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Reloading surname dataset and vectorizer from file\")\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.data_file, args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating a new vectorizer\")\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.data_file)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a two-layer MLP\n",
      "Number of features: 77\n",
      "Number of hidden units: 300\n",
      "Number of output classes: 18\n",
      "Dropout Probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "classifier = Two_Layer_MLP(num_features=len(vectorizer.surname_vocab),\n",
    "                          hidden_dim=args.hidden_dim,\n",
    "                          output_dim=len(vectorizer.nationality_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving classifier to cpu\n",
      "Moving class weights of dataset object to cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Moving classifier to {}\".format(args.device))\n",
    "classifier = classifier.to(args.device)\n",
    "print(\"Moving class weights of dataset object to {}\".format(args.device))\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 2019-09-04 10:43:04,974 : Epoch 1 - Validation Accuracy 38.88\n",
      "INFO : 2019-09-04 10:43:07,645 : Epoch 2 - Validation Accuracy 38.56\n",
      "INFO : 2019-09-04 10:43:10,354 : Epoch 3 - Validation Accuracy 39.06\n",
      "INFO : 2019-09-04 10:43:13,071 : Epoch 4 - Validation Accuracy 38.44\n",
      "INFO : 2019-09-04 10:43:15,798 : Epoch 5 - Validation Accuracy 40.62\n",
      "INFO : 2019-09-04 10:43:18,462 : Epoch 6 - Validation Accuracy 39.00\n",
      "INFO : 2019-09-04 10:43:21,149 : Epoch 7 - Validation Accuracy 39.56\n",
      "INFO : 2019-09-04 10:43:23,823 : Epoch 8 - Validation Accuracy 35.94\n",
      "INFO : 2019-09-04 10:43:26,532 : Epoch 9 - Validation Accuracy 38.94\n",
      "INFO : 2019-09-04 10:43:29,247 : Epoch 10 - Validation Accuracy 39.69\n",
      "INFO : 2019-09-04 10:43:32,240 : Epoch 11 - Validation Accuracy 38.69\n",
      "INFO : 2019-09-04 10:43:35,109 : Epoch 12 - Validation Accuracy 38.62\n",
      "INFO : 2019-09-04 10:43:37,854 : Epoch 13 - Validation Accuracy 40.81\n",
      "INFO : 2019-09-04 10:43:40,621 : Epoch 14 - Validation Accuracy 40.94\n",
      "INFO : 2019-09-04 10:43:43,426 : Epoch 15 - Validation Accuracy 40.44\n",
      "INFO : 2019-09-04 10:43:46,213 : Epoch 16 - Validation Accuracy 39.00\n",
      "INFO : 2019-09-04 10:43:48,988 : Epoch 17 - Validation Accuracy 40.63\n",
      "INFO : 2019-09-04 10:43:51,662 : Epoch 18 - Validation Accuracy 39.31\n",
      "INFO : 2019-09-04 10:43:54,392 : Epoch 19 - Validation Accuracy 39.62\n",
      "INFO : 2019-09-04 10:43:57,077 : Epoch 20 - Validation Accuracy 39.44\n",
      "INFO : 2019-09-04 10:43:59,788 : Epoch 21 - Validation Accuracy 39.62\n",
      "INFO : 2019-09-04 10:44:02,504 : Epoch 22 - Validation Accuracy 39.63\n",
      "INFO : 2019-09-04 10:44:05,228 : Epoch 23 - Validation Accuracy 40.31\n",
      "INFO : 2019-09-04 10:44:07,941 : Epoch 24 - Validation Accuracy 41.56\n",
      "INFO : 2019-09-04 10:44:10,617 : Epoch 25 - Validation Accuracy 40.69\n",
      "INFO : 2019-09-04 10:44:13,349 : Epoch 26 - Validation Accuracy 39.75\n",
      "INFO : 2019-09-04 10:44:16,025 : Epoch 27 - Validation Accuracy 39.94\n",
      "INFO : 2019-09-04 10:44:18,736 : Epoch 28 - Validation Accuracy 38.81\n",
      "INFO : 2019-09-04 10:44:21,467 : Epoch 29 - Validation Accuracy 41.31\n",
      "INFO : 2019-09-04 10:44:24,188 : Epoch 30 - Validation Accuracy 40.31\n",
      "INFO : 2019-09-04 10:44:26,842 : Epoch 31 - Validation Accuracy 39.19\n",
      "INFO : 2019-09-04 10:44:29,553 : Epoch 32 - Validation Accuracy 39.50\n",
      "INFO : 2019-09-04 10:44:32,268 : Epoch 33 - Validation Accuracy 40.62\n",
      "INFO : 2019-09-04 10:44:35,149 : Epoch 34 - Validation Accuracy 40.56\n",
      "INFO : 2019-09-04 10:44:37,941 : Epoch 35 - Validation Accuracy 40.94\n",
      "INFO : 2019-09-04 10:44:40,724 : Epoch 36 - Validation Accuracy 39.44\n",
      "INFO : 2019-09-04 10:44:43,511 : Epoch 37 - Validation Accuracy 41.19\n",
      "INFO : 2019-09-04 10:44:46,302 : Epoch 38 - Validation Accuracy 40.38\n",
      "INFO : 2019-09-04 10:44:49,098 : Epoch 39 - Validation Accuracy 40.56\n",
      "INFO : 2019-09-04 10:44:51,884 : Epoch 40 - Validation Accuracy 39.94\n",
      "INFO : 2019-09-04 10:44:54,679 : Epoch 41 - Validation Accuracy 40.00\n",
      "INFO : 2019-09-04 10:44:57,462 : Epoch 42 - Validation Accuracy 40.87\n",
      "INFO : 2019-09-04 10:45:00,213 : Epoch 43 - Validation Accuracy 40.50\n",
      "INFO : 2019-09-04 10:45:02,994 : Epoch 44 - Validation Accuracy 42.25\n",
      "INFO : 2019-09-04 10:45:05,703 : Epoch 45 - Validation Accuracy 39.81\n",
      "INFO : 2019-09-04 10:45:08,472 : Epoch 46 - Validation Accuracy 40.31\n",
      "INFO : 2019-09-04 10:45:11,263 : Epoch 47 - Validation Accuracy 40.31\n",
      "INFO : 2019-09-04 10:45:14,031 : Epoch 48 - Validation Accuracy 40.00\n",
      "INFO : 2019-09-04 10:45:16,809 : Epoch 49 - Validation Accuracy 41.75\n",
      "INFO : 2019-09-04 10:45:19,568 : Epoch 50 - Validation Accuracy 40.00\n",
      "INFO : 2019-09-04 10:45:22,414 : Epoch 51 - Validation Accuracy 40.50\n",
      "INFO : 2019-09-04 10:45:25,145 : Epoch 52 - Validation Accuracy 40.25\n",
      "INFO : 2019-09-04 10:45:27,895 : Epoch 53 - Validation Accuracy 40.38\n",
      "INFO : 2019-09-04 10:45:30,623 : Epoch 54 - Validation Accuracy 39.38\n",
      "INFO : 2019-09-04 10:45:33,387 : Epoch 55 - Validation Accuracy 39.69\n",
      "INFO : 2019-09-04 10:45:36,165 : Epoch 56 - Validation Accuracy 40.37\n",
      "INFO : 2019-09-04 10:45:38,903 : Epoch 57 - Validation Accuracy 41.62\n",
      "INFO : 2019-09-04 10:45:42,003 : Epoch 58 - Validation Accuracy 40.31\n",
      "INFO : 2019-09-04 10:45:44,979 : Epoch 59 - Validation Accuracy 40.88\n",
      "INFO : 2019-09-04 10:45:47,782 : Epoch 60 - Validation Accuracy 40.94\n",
      "INFO : 2019-09-04 10:45:50,980 : Epoch 61 - Validation Accuracy 41.31\n",
      "INFO : 2019-09-04 10:45:53,675 : Epoch 62 - Validation Accuracy 38.94\n",
      "INFO : 2019-09-04 10:45:56,316 : Epoch 63 - Validation Accuracy 40.38\n",
      "INFO : 2019-09-04 10:45:59,017 : Epoch 64 - Validation Accuracy 41.00\n",
      "INFO : 2019-09-04 10:46:01,710 : Epoch 65 - Validation Accuracy 38.94\n",
      "INFO : 2019-09-04 10:46:04,390 : Epoch 66 - Validation Accuracy 40.62\n",
      "INFO : 2019-09-04 10:46:07,096 : Epoch 67 - Validation Accuracy 40.19\n",
      "INFO : 2019-09-04 10:46:09,796 : Epoch 68 - Validation Accuracy 41.00\n",
      "INFO : 2019-09-04 10:46:12,492 : Epoch 69 - Validation Accuracy 40.62\n",
      "INFO : 2019-09-04 10:46:15,213 : Epoch 70 - Validation Accuracy 40.81\n",
      "INFO : 2019-09-04 10:46:17,971 : Epoch 71 - Validation Accuracy 41.06\n",
      "INFO : 2019-09-04 10:46:20,666 : Epoch 72 - Validation Accuracy 40.94\n",
      "INFO : 2019-09-04 10:46:23,434 : Epoch 73 - Validation Accuracy 40.87\n",
      "INFO : 2019-09-04 10:46:26,259 : Epoch 74 - Validation Accuracy 40.69\n",
      "INFO : 2019-09-04 10:46:29,007 : Epoch 75 - Validation Accuracy 40.25\n",
      "INFO : 2019-09-04 10:46:31,795 : Epoch 76 - Validation Accuracy 40.69\n",
      "INFO : 2019-09-04 10:46:34,574 : Epoch 77 - Validation Accuracy 40.75\n",
      "INFO : 2019-09-04 10:46:37,431 : Epoch 78 - Validation Accuracy 41.31\n",
      "INFO : 2019-09-04 10:46:40,265 : Epoch 79 - Validation Accuracy 41.00\n",
      "INFO : 2019-09-04 10:46:43,047 : Epoch 80 - Validation Accuracy 40.06\n",
      "INFO : 2019-09-04 10:46:45,880 : Epoch 81 - Validation Accuracy 41.12\n",
      "INFO : 2019-09-04 10:46:48,735 : Epoch 82 - Validation Accuracy 41.12\n",
      "INFO : 2019-09-04 10:46:51,734 : Epoch 83 - Validation Accuracy 39.25\n",
      "INFO : 2019-09-04 10:46:54,564 : Epoch 84 - Validation Accuracy 41.69\n",
      "INFO : 2019-09-04 10:46:57,392 : Epoch 85 - Validation Accuracy 39.94\n",
      "INFO : 2019-09-04 10:47:00,175 : Epoch 86 - Validation Accuracy 41.62\n",
      "INFO : 2019-09-04 10:47:02,988 : Epoch 87 - Validation Accuracy 41.94\n",
      "INFO : 2019-09-04 10:47:05,714 : Epoch 88 - Validation Accuracy 40.25\n",
      "INFO : 2019-09-04 10:47:08,642 : Epoch 89 - Validation Accuracy 40.44\n",
      "INFO : 2019-09-04 10:47:11,417 : Epoch 90 - Validation Accuracy 40.31\n",
      "INFO : 2019-09-04 10:47:14,178 : Epoch 91 - Validation Accuracy 40.06\n",
      "INFO : 2019-09-04 10:47:16,911 : Epoch 92 - Validation Accuracy 39.81\n",
      "INFO : 2019-09-04 10:47:19,680 : Epoch 93 - Validation Accuracy 39.88\n",
      "INFO : 2019-09-04 10:47:22,452 : Epoch 94 - Validation Accuracy 39.44\n",
      "INFO : 2019-09-04 10:47:25,245 : Epoch 95 - Validation Accuracy 40.50\n",
      "INFO : 2019-09-04 10:47:28,013 : Epoch 96 - Validation Accuracy 40.81\n",
      "INFO : 2019-09-04 10:47:30,807 : Epoch 97 - Validation Accuracy 41.00\n",
      "INFO : 2019-09-04 10:47:33,559 : Epoch 98 - Validation Accuracy 39.94\n",
      "INFO : 2019-09-04 10:47:36,348 : Epoch 99 - Validation Accuracy 40.69\n",
      "INFO : 2019-09-04 10:47:39,416 : Epoch 100 - Validation Accuracy 41.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 2s, sys: 1.61 s, total: 27min 3s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch_index\n",
    "        dataset.set_split(\"train\")\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # step 2. compute output\n",
    "            predictions = classifier.forward(batch_dict[\"x_surname\"])\n",
    "            # step 3. compute loss\n",
    "            loss = loss_func(predictions, batch_dict[\"y_nationality\"])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            # step 4. compute gradients given loss\n",
    "            loss.backward()\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_t = compute_accuracy(predictions, batch_dict[\"y_nationality\"])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "        \n",
    "        # now compute on validation set\n",
    "        dataset.set_split(\"val\")\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.eval()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            predictions = classifier.forward(batch_dict[\"x_surname\"])\n",
    "            loss = loss_func(predictions, batch_dict[\"y_nationality\"])\n",
    "            loss_t = loss.to(\"cpu\").item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            acc_t = compute_accuracy(predictions, batch_dict[\"y_nationality\"])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "        train_state[\"val_loss\"].append(running_loss)\n",
    "        train_state[\"val_acc\"].append(running_acc)\n",
    "        \n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "        scheduler.step(train_state[\"val_loss\"][-1])\n",
    "        \n",
    "        logging.info(\"Epoch {0:d} - Validation Accuracy {1:.2f}\"\\\n",
    "                     .format(epoch_index+1, train_state['val_acc'][-1]))\n",
    "        if train_state[\"stop_early\"]:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting Loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state[\"model_filename\"]))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split(\"test\")\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    predictions = classifier.forward(batch_dict[\"x_surname\"])\n",
    "    loss = loss_func(predictions, batch_dict[\"y_nationality\"])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "    acc_t = compute_accuracy(predictions, batch_dict[\"y_nationality\"])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    \n",
    "train_state[\"test_loss\"] = running_loss\n",
    "train_state[\"test_acc\"] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.933490538597107\n",
      "Test Accuracy: 39.18750000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {}\".format(train_state[\"test_loss\"]))\n",
    "print(\"Test Accuracy: {}\".format(train_state[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_args = Namespace(\n",
    "    data_file = \"data/surnames_with_splits.csv\",\n",
    "    vectorizer_file = \"cnn_surname_vectorizer.json\",\n",
    "    model_state_file = \"cnn_model.pth\",\n",
    "    save_dir = \"data\",\n",
    "    # Model hyper parameters\n",
    "    hidden_dim=100,\n",
    "    num_channels=256,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    dropout_p=0.1,\n",
    "    # Runtime options\n",
    "    cuda=False,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    catch_keyboard_interrupt=True\n",
    ")\n",
    "cnn_args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating a new vectorizer\n",
      "Initialized dataset split to train\n"
     ]
    }
   ],
   "source": [
    "if cnn_args.reload_from_files:\n",
    "    print(\"Reloading surname dataset and vectorizer from file\")\n",
    "    cnn_dataset = CNN_SurnameDataset.load_dataset_and_load_vectorizer(cnn_args.data_file, cnn_args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating a new vectorizer\")\n",
    "    cnn_dataset = CNN_SurnameDataset.load_dataset_and_make_vectorizer(cnn_args.data_file)\n",
    "    cnn_dataset.save_vectorizer(cnn_args.vectorizer_file)\n",
    "\n",
    "cnn_vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized CNN\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CNN_Model(initial_num_channels=len(cnn_vectorizer.surname_vocab),\n",
    "                     num_classes=len(cnn_vectorizer.nationality_vocab),\n",
    "                     num_channels=cnn_args.num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving classifier to cpu\n",
      "Moving class weights of dataset object to cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Moving classifier to {}\".format(cnn_args.device))\n",
    "cnn_model = cnn_model.to(cnn_args.device)\n",
    "print(\"Moving class weights of dataset object to {}\".format(cnn_args.device))\n",
    "cnn_dataset.class_weights = cnn_dataset.class_weights.to(cnn_args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(cnn_dataset.class_weights)\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=cnn_args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(cnn_args)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(cnn_args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch_index\n",
    "        cnn_dataset.set_split(\"train\")\n",
    "        batch_generator = generate_batches(cnn_dataset, batch_size=cnn_args.batch_size, device=cnn_args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        cnn_model.train()\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # step 2. compute output\n",
    "            predictions = cnn_model.forward(batch_dict[\"x_surname\"])\n",
    "            # step 3. compute loss\n",
    "            loss = loss_func(predictions, batch_dict[\"y_nationality\"])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            # step 4. compute gradients given loss\n",
    "            loss.backward()\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_t = compute_accuracy(predictions, batch_dict[\"y_nationality\"])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "        \n",
    "        # now compute on validation set\n",
    "        cnn_dataset.set_split(\"val\")\n",
    "        batch_generator = generate_batches(cnn_dataset, batch_size=cnn_args.batch_size, device=cnn_args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        cnn_model.eval()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            predictions = cnn_model.forward(batch_dict[\"x_surname\"])\n",
    "            loss = loss_func(predictions, batch_dict[\"y_nationality\"])\n",
    "            loss_t = loss.to(\"cpu\").item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            acc_t = compute_accuracy(predictions, batch_dict[\"y_nationality\"])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "        train_state[\"val_loss\"].append(running_loss)\n",
    "        train_state[\"val_acc\"].append(running_acc)\n",
    "        \n",
    "        train_state = update_train_state(args=cnn_args, model=cnn_model, train_state=train_state)\n",
    "        scheduler.step(train_state[\"val_loss\"][-1])\n",
    "        \n",
    "        logging.info(\"Epoch {0:d} - Validation Accuracy {1:.2f}\"\\\n",
    "                     .format(epoch_index+1, train_state['val_acc'][-1]))\n",
    "        if train_state[\"stop_early\"]:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.load_state_dict(torch.load(train_state[\"model_filename\"]))\n",
    "\n",
    "cnn_model = cnn_model.to(cnn_args.device)\n",
    "dataset.class_weights = cnn_dataset.class_weights.to(cnn_args.device)\n",
    "loss_func = nn.CrossEntropyLoss(cnn_dataset.class_weights)\n",
    "\n",
    "cnn_dataset.set_split(\"test\")\n",
    "batch_generator = generate_batches(cnn_dataset, batch_size=cnn_args.batch_size, device=cnn_args.device)\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "cnn_model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    predictions = cnn_model.forward(batch_dict[\"x_surname\"])\n",
    "    loss = loss_func(predictions, batch_dict[\"y_nationality\"])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "    acc_t = compute_accuracy(predictions, batch_dict[\"y_nationality\"])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    \n",
    "train_state[\"test_loss\"] = running_loss\n",
    "train_state[\"test_acc\"] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test loss: {}\".format(train_state[\"test_loss\"]))\n",
    "print(\"Test Accuracy: {}\".format(train_state[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Inference on New Examples\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    \"\"\"\n",
    "    Predict the nationality from a new surname\n",
    "    \n",
    "    Args:\n",
    "        surname (str): the surname to classifier\n",
    "        classifier (Two_Layer_MLP_Layer_MLP): an instance of classifier\n",
    "        vectorizer (SurnameVectorizer): an instance of vectorizer\n",
    "    Returns:\n",
    "        dictionary with the most likely nationality and its corresponding probability\n",
    "    \"\"\"\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).view(1,-1)\n",
    "    result = classifier.forward(vectorized_surname, apply_softmax=True)\n",
    "    \n",
    "    probability_vals, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "    predicted_nationality = vectorizer.nationality_vocab.get_token(index)\n",
    "    probability_value = probability_vals.item()\n",
    "    \n",
    "    return {\"nationality\": predicted_nationality, \"probability\": probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cnn_predict_nationality(surname, classifier, vectorizer):\n",
    "    \"\"\"Predict the nationality from a new surname\n",
    "    \n",
    "    Args:\n",
    "        surname (str): the surname to classifier\n",
    "        classifier (SurnameClassifer): an instance of the classifier\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "    Returns:\n",
    "        a dictionary with the most likely nationality and its probability\n",
    "    \"\"\"\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(0)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a surname: Corrado\n",
      "Corrado -> Portuguese (p=0.49)\n"
     ]
    }
   ],
   "source": [
    "new_surname = input(\"Enter a surname: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:.2f})\".format(new_surname, prediction[\"nationality\"],prediction[\"probability\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surname = input(\"Enter a surname: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "prediction = cnn_predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:.2f})\".format(new_surname, prediction[\"nationality\"],prediction[\"probability\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict_topn_nationalities(new_surname, classifier, vectorizer, topn=5):\n",
    "    vectorized_surname = vectorizer.vectorize(new_surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).view(1,-1)\n",
    "    prediction_vector = classifier.forward(vectorized_surname, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk( prediction_vector, k=topn )\n",
    "    \n",
    "    probability_values = probability_values.detach().numpy()[0]\n",
    "    indices = indices.detach().numpy()[0]\n",
    "    \n",
    "    results = []\n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        nationality = vectorizer.nationality_vocab.get_token(index)\n",
    "        results.append({\"nationality\": nationality, \"probability\": prob_value})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a surname: Corrado\n",
      "Top predictions:\n",
      "================\n",
      "Corrado -> Portuguese (p=0.39)\n",
      "Corrado -> Italian (p=0.20)\n",
      "Corrado -> Irish (p=0.10)\n",
      "Corrado -> English (p=0.08)\n",
      "Corrado -> French (p=0.07)\n"
     ]
    }
   ],
   "source": [
    "new_surname = input(\"Enter a surname: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "predictions = predict_topn_nationalities(new_surname, classifier, vectorizer)\n",
    "print(\"Top predictions:\")\n",
    "print(\"================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:.2f})\".format(new_surname, prediction[\"nationality\"],prediction[\"probability\"] ))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "277.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
