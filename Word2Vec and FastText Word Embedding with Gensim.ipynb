{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to introduce two SOTA word embedding methods, **Word2Vec** and **FastText**, using Gensim\n",
    "\n",
    "# Traditional Approach\n",
    "***\n",
    "A traditional way of representing words is using a one-hot vector, so each word gets its own basis vector. The length of this one-hot vector is always equal to the size of the unique vocabulary in the corpus. \n",
    "\n",
    "This representation is simple and easy to implement, but it does not embed any sort of semantic meaning between two words given their one-hot representation. For example, assuming you assign index positions to words in alphabetical order, the words \"endure\" and \"tolerate\" would be very far away in their one-hot encoded space (\"en\" is far from \"to\", alphabetically). However, we ideally want to embed words in a space where these two words are close to one another. \n",
    "\n",
    "# Word2Vec\n",
    "***\n",
    "Word2Vec is an efficient solution to these problems that leverages the context of the target words. We use the surrounding words to represent target words (distributional hypothesis) using a Neural Network whose hidden layer encodes the word representation.\n",
    "\n",
    "There are two variations of Word2Vec: **Skip-gram** and **Continuous Bag of Words**\n",
    "\n",
    "## Skip-gram\n",
    "***\n",
    "In skip-gram, the input is the target word, and the outputs are the words surrounding the target words. For example, consider the sentence \"I have a cute dog\". The input to the neural network could be the word \"a\", and the output of the neural network could be [\"I\",\"have\",\"cute\",\"dog\"] (assuming a window of length 5). The input and output data are of the same dimension, and use a one-hot encoding. The network is typically shallow and contains 1 hidden layer whose number of nodes is equal to the dimension of the embedding space. Typically, the size of the hidden layer is smaller than the input or output vector size (to have a smaller dimensional embedding space). At the end of the output layer, a softmax function is applied so that each element of the output vector describes the probability of seeing that word in the target word's context.\n",
    "\n",
    "The word embedding for the target words is obtained by extracting the hidden layers after feeding the one-hot representation of that word into the network.\n",
    "\n",
    "With skip-gram, the dimension of the representation decreases from the vocabulary size to the length of the hidden layer (N). In addition, the vectors are more meaningful in terms of the probability of being in the same context as another word (again, by assumption of the distributional hypothesis). Under this structure, the vector difference obtained by subtracting two related words sometimes expresses a meaningful concept such as gender or verb tense (king+woman-queen = man). \n",
    "\n",
    "## (CBOW) Continuous Bag of Words\n",
    "***\n",
    "The continuous Bag of Words is very similar to the skip-gram, except it swaps the input and output. Given some context, we predict what the target word is. \n",
    "\n",
    "The largest difference between the skip-gram and CBOW methods is the way the word vectors are generated. For CBOW, the examples with the target word are fed into the networks, and one takes the average of the extracted hidden layer. For example, assume we have two sentences: [\"He is a nice guy\",\"She is a wise queen\"]. To compute the word representation for the word \"a\", we feed these two sentences into the neural network and we take the average of the values in the hidden layer. \n",
    "\n",
    "In skip-gram, we feed in one target word as a one-hot vector and we get back a context vector. \n",
    "\n",
    "# Word2Vec Implementation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/yelp/raw_train.csv\", header=None,names=[\"sentiment\",\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1  Unfortunately, the frustration of being Dr. Go...\n",
       "1          2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2          1  I don't know what Dr. Goldberg was like before...\n",
       "3          1  I'm writing this review to give you a heads up...\n",
       "4          2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do some preprocessing to make sure we can learn contexts a little better, and then we tokenize it, so that our final version can be read in by the Word2Vec model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review):\n",
    "    # remove parentheses\n",
    "    review = re.sub(r\"\\([^)]*\\)\", \"\", review)\n",
    "    # remove any non-alphanumeric characters and make lowercase\n",
    "    review = re.sub(r\"[^a-z0-9]+\", \" \", review.lower()).split()\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cleaned_review\"] = data.review.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=data[\"cleaned_review\"], size=100, window=5, min_count=5,\\\n",
    "                workers=8, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guy', 0.858910322189331),\n",
       " ('woman', 0.8234066963195801),\n",
       " ('gentleman', 0.8075653910636902),\n",
       " ('lady', 0.8026242852210999),\n",
       " ('girl', 0.7914206385612488),\n",
       " ('dude', 0.7794543504714966),\n",
       " ('gal', 0.7622164487838745),\n",
       " ('gent', 0.7058786749839783),\n",
       " ('bouncer', 0.6754287481307983),\n",
       " ('gentlemen', 0.6743558049201965)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest challenges for Word2Vec is that it cannot represent words it didn't see in the training set. For this reason, we can use **FastText**\n",
    "\n",
    "# FastText\n",
    "***\n",
    "FastText is an extension to Word2Vec. Instead of feeding whole individual words into the Neural Network, FastText instead breaks each word into several n-grams (sub parts of word). For example, the tri-grams for \"apple\" are [\"app\",\"ppl\",\"ple\"]. Under this featurization, the word embedding vector for apple will be the sum of these n-grams. After training the Neural Network, rare words can now be properly represented since it is highly likely that some of their n-grams will have appeared in the training set. \n",
    "# FastText Implementation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = FastText(sentences=data[\"cleaned_review\"], size=100, window=5, \\\n",
    "                          min_count=5, workers=8, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try it with a word that does not appear in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word in training set: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('antibiotics', 0.6070331335067749),\n",
       " ('pregnancies', 0.5956847667694092),\n",
       " ('surgeries', 0.5834647417068481),\n",
       " ('pregnancy', 0.574320912361145),\n",
       " ('resurgence', 0.573951244354248),\n",
       " ('innocence', 0.5651587247848511),\n",
       " ('invasion', 0.5600395798683167),\n",
       " ('illness', 0.5597577095031738),\n",
       " ('symptoms', 0.557685375213623),\n",
       " ('probiotics', 0.5571395754814148)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_not_in_training_set = \"Gastroenteritis\"\n",
    "print(\"Word in training set: {}\"\\\n",
    "      .format(word_not_in_training_set_not_in_training_set in model.wv.vocab.keys()))\n",
    "fasttext_model.wv.most_similar(word_not_in_training_set)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
