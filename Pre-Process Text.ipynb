{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have gathered the text, the next stage is about cleaning and consolidating the text.\n",
    "\n",
    "We need to standardize the text to remove the noise so that an efficient analysis can be performed to derive meaningful insights.\n",
    "\n",
    "THe cleaning and processing of text is highly dependent on the nature of the NLP project. If numbers are important for your project, you might have to do a different set of cleaning and processing.\n",
    "\n",
    "We will go through the following techniques:\n",
    "\n",
    "1. Convert text to lowercase\n",
    "2. Tokenize paragraphs to sentences\n",
    "3. Tokenize sentences to words\n",
    "4. Remove numbers\n",
    "5. Remove punctuation\n",
    "6. Remove stop words\n",
    "7. Remove whitespaces\n",
    "\n",
    "We will be using the `nltk` library in order to perform this normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text to lowercase\n",
    "***\n",
    "Change the case of the words to ensure every word is lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an nlp article of fintechexplained\n"
     ]
    }
   ],
   "source": [
    "text = 'This is an NLP article of FinTechExplained'\n",
    "\n",
    "lower_case_text = text.lower()\n",
    "\n",
    "print(lower_case_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Paragraphs to Sentences\n",
    "***\n",
    "Use the `nltk` library to perform tokenization. We can use the `PunktSentenceTokenize` model to perform sentence-level tokenization by determining punctuation and character marking at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FinTechExplained aims to explain how text processing works.', 'Once we have gathered the text, the next stage is about cleaning and \\nconsolidating the text.', 'It is important to ensure the text is standardised\\nand the noise is removed so that efficient analysis can be performed on the\\ntext to derive meaningful insights.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "paragraph = '''FinTechExplained aims to explain how text processing works.  \n",
    "Once we have gathered the text, the next stage is about cleaning and \n",
    "consolidating the text. It is important to ensure the text is standardised\n",
    "and the noise is removed so that efficient analysis can be performed on the\n",
    "text to derive meaningful insights.'''\n",
    "\n",
    "my_list = sent_tokenize( paragraph )\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Sentences to Words\n",
    "***\n",
    "We can use the `TreebankWordTokenizer` from `nltk` to tokenize sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FinTechExplained', 'aims', 'to', 'explain', 'how', 'text', 'processing', 'works.', 'Once', 'we', 'have', 'gathered', 'the', 'text', ',', 'the', 'next', 'stage', 'is', 'about', 'cleaning', 'and', 'consolidating', 'the', 'text.', 'It', 'is', 'important', 'to', 'ensure', 'the', 'text', 'is', 'standardised', 'and', 'the', 'noise', 'is', 'removed', 'so', 'that', 'efficient', 'analysis', 'can', 'be', 'performed', 'on', 'the', 'text', 'to', 'derive', 'meaningful', 'insights', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Numbers\n",
    "***\n",
    "Use a regular expression to remove all numbers from a given string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinTechExplained\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "result = re.sub(r'\\d+', '','909FinTechExplained9876')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation\n",
    "***\n",
    "Now we can remove punctuation from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'Are', 'Reading', 'FinTechExplained', 'NLP']\n",
      "Cleaned sentence: YouAreReadingFinTechExplainedNLP\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "words = ['You','Are','Reading','FinTechExplained','!','NLP','.']\n",
    "\n",
    "clean_words = [w for w in words if w not in punctuation]\n",
    "\n",
    "print(clean_words)\n",
    "print(\"Cleaned sentence:\", \"\".join(clean_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words\n",
    "***\n",
    "Stop words: \"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\" are some common English stop words.\n",
    "\n",
    "Use `nltk` to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FinTechExplained', 'important', 'publication']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "text = 'FinTechExplained is an important publication'\n",
    "words = nltk.word_tokenize( text )\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "clean_words = [w for w in words if w not in stopwords]\n",
    "\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Whitespaces\n",
    "***\n",
    "How to remove whitespaces such as space, tab, carriage return, line feeds, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FinTechExplained', 'Is', 'A', 'Publication.', 'This', 'is', 'about', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'FinTechExplained Is A    Publication. \\n This is about NLP'\n",
    "\n",
    "splitted_words = sentence.split()\n",
    "\n",
    "print(splitted_words)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
